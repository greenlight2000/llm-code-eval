Pipeline of evaluating LLM's performance of code optimization:

1. Run the inference scripts in the `evaluators` folder to get the inference results p the targeted model. The inference results ({time/mem}_code_opt_inference_{modelname}.jsonl) will be saved in the `results/raw` folder.
2. Run `python save_opt_codes.py --code_test_data_name mem_code_opt_inference_{modelname}.jsonl --codes_dir_name {modelname}_opt_parse --opt_type mem --parse_code` and `python save_opt_codes.py --code_test_data_name time_code_opt_inference_{modelname}.jsonl --codes_dir_name {modelname}_opt_parse --opt_type time --parse_code` to save the parsed optimized codes in the `results/ans/{modelname}_opt_parse` folder.
3. Run `python test_opt_codes.py --code_test_data_name mem_code_opt_inference_{modelname}.jsonl --codes_dir_name {modelname}_opt_parse --opt_type mem` and `python test_opt_codes.py --code_test_data_name time_code_opt_inference_{modelname}.jsonl --codes_dir_name {modelname}_opt_parse --opt_type time` to execute the parsed optimized codes and get the execution time and memory usage. The performance metrics of each code snippet will be saved back to the parent folder of the code file under `results/ans/{modelname}_opt_parse`.
4. Go to `scores` folder and run `python cal_metrics.py --codes_dir_name palm_{modelname}_parse > cal_{modelname}_metrics.log`. This will calculate the pass@5 and opt@5 scores of the targeted model.